{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97f31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import torch.nn \n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F \n",
    "import torchvision.utils as utils\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1ef9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "horses_images_names = []\n",
    "bikes_images_names = []\n",
    "\n",
    "hFolder = \"Assignment2_BikeHorses/Horses\"\n",
    "bFolder = \"Assignment2_BikeHorses/Bikes\"\n",
    "\n",
    "for image in os.listdir(hFolder):\n",
    "    if(image.endswith(\".jpg\")):\n",
    "        horses_images_names.append(image)\n",
    "        \n",
    "print(len(horses_images_names))\n",
    "\n",
    "for image in os.listdir(bFolder):\n",
    "    if(image.endswith(\".jpg\")):\n",
    "        bikes_images_names.append(image)\n",
    "\n",
    "print(len(bikes_images_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2053dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([        # Defining a variable transforms\n",
    "transforms.Resize(256),                # Resize the image to 256×256 pixels\n",
    "transforms.CenterCrop(224),            # Crop the image to 224×224 pixels about the center\n",
    "transforms.ToTensor(),                 # Convert the image to PyTorch Tensor data type\n",
    "transforms.Normalize(                  # Normalize the image\n",
    "mean=[0.485, 0.456, 0.406],            # Mean and std of image as also used when training the network\n",
    "std=[0.229, 0.224, 0.225]      \n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9953924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horses_images = []\n",
    "bikes_images = []\n",
    "\n",
    "for i in range(len(horses_images_names)):\n",
    "    img = Image.open(hFolder + \"/\" + horses_images_names[i])\n",
    "    transformed_img = transform(img)\n",
    "    batch_img = torch.unsqueeze(transformed_img, 0)\n",
    "    horses_images.append(batch_img)    \n",
    "    \n",
    "for i in range(len(bikes_images_names)):\n",
    "    img = Image.open(bFolder + \"/\" + bikes_images_names[i])\n",
    "    transformed_img = transform(img)\n",
    "    batch_img = torch.unsqueeze(transformed_img, 0)\n",
    "    bikes_images.append(batch_img) \n",
    "    \n",
    "horses_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0216b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(horses_images[0].shape)\n",
    "print(bikes_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaf0afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\Saikrishna/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca127919a777487cb56f41de85096e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/233M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet = models.alexnet(pretrained = True)\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7899eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eefe224",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images = horses_images\n",
    "preprocessed_images.extend(bikes_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41056258",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in preprocessed_images:\n",
    "    with torch.no_grad():\n",
    "        feature = alexnet(img).detach().numpy()\n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "614e1a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b40902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(features)):\n",
    "    features[i] = features[i].T.reshape((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c26d061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features1 = np.array(features)\n",
    "features1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e46a4",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15702e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "labels = np.zeros(len(preprocessed_images))\n",
    "for i in range(len(preprocessed_images)):\n",
    "     if(i<99):\n",
    "        labels[i] = 1\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab2cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9e136",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fa1f20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.0005, class_weight='balanced', kernel='linear')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVC(C=0.0005,kernel='linear', class_weight = 'balanced', gamma = 'scale')\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1b36070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "acc = model.score(x_test,y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ebb7258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_lr = LogisticRegression(max_iter = 1000)\n",
    "model_lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de35077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "acc1 = model_lr.score(x_test,y_test)\n",
    "print(acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1461ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
